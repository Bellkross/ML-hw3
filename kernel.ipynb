{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "4144b1bb1f9a38697637b2a696baf56bcc12e8dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.2.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "\u001b[31mmxnet 1.3.0.post0 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.15.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mkmeans-smote 0.1.0 has requirement imbalanced-learn<0.4,>=0.3.1, but you'll have imbalanced-learn 0.5.0.dev0 which is incompatible.\u001b[0m\n",
      "\u001b[31mkmeans-smote 0.1.0 has requirement numpy<1.15,>=1.13, but you'll have numpy 1.15.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 0.4.1.post2 which is incompatible.\u001b[0m\n",
      "\u001b[31manaconda-client 1.7.2 has requirement python-dateutil>=2.6.1, but you'll have python-dateutil 2.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mimbalanced-learn 0.5.0.dev0 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
      "Requirement already satisfied: torchtext in /opt/conda/lib/python3.6/site-packages (0.3.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from torchtext) (2.18.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from torchtext) (0.4.1.post2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchtext) (1.15.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from torchtext) (4.26.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->torchtext) (2018.10.15)\n",
      "\u001b[31mmxnet 1.3.0.post0 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.15.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mkmeans-smote 0.1.0 has requirement imbalanced-learn<0.4,>=0.3.1, but you'll have imbalanced-learn 0.5.0.dev0 which is incompatible.\u001b[0m\n",
      "\u001b[31mkmeans-smote 0.1.0 has requirement numpy<1.15,>=1.13, but you'll have numpy 1.15.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mfastai 0.7.0 has requirement torch<0.4, but you'll have torch 0.4.1.post2 which is incompatible.\u001b[0m\n",
      "\u001b[31manaconda-client 1.7.2 has requirement python-dateutil>=2.6.1, but you'll have python-dateutil 2.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mimbalanced-learn 0.5.0.dev0 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e08fa0bb40694e4df51f7d833fb62392e604af31"
   },
   "source": [
    "# <a name=\"content_list\">Home work III</a>\n",
    "   1. [Preprocessing of text data](#preprocessing)\n",
    "       1. [Loading data](#loading_data)\n",
    "       2. [Loading embeddings](#loading_embeddings)\n",
    "       3. [Vocabulary](#vocabulary)\n",
    "           1. [Contractions](#contractions)\n",
    "           2. [Special characters](#special_characters)\n",
    "           3. [Spelling mistakes](#spelling_mistakes)\n",
    "   2. [Simple feed-forward neural network model](#simplenn)\n",
    "   3. [TF-IDF  vectorizer and Logistic regression](#logit)\n",
    "   4. [Summary](#summary)\n",
    "\n",
    "Course: Introduction in machine learning\n",
    "\n",
    "Lecturer: Taras Lehinevych\n",
    "\n",
    "Author: Kyryll Vasylenko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45bffa0450dd1f31e6116cfaf47761e19cbc1c9f"
   },
   "source": [
    "# <a name=\"preprocessing\">Preprocessing of text data</a>\n",
    "\n",
    "As information for data preprocessing I used Deiters kernel \"How to: Preprocessing when using embeddings\"\n",
    "\n",
    "I for data preprocessing I used Glove, Paragram, FastText embeddings because they are the most popular in examples for model training\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "152aa4f0891da981f1c5c954e5abd80a7324f641"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9849de10c9ec4800c383e359870403d06a39fcaa"
   },
   "source": [
    "## <a name=\"loading_data\">Loading data</a>\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "afc7c2025606f94aac6500b8b7d3ffec29bbf6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts:  1362492\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\").drop('target', axis=1)\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "df = pd.concat([train ,test])\n",
    "\n",
    "print(\"Number of texts: \", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "884f4af1722aa6286b01feefe154a02f9b604fb9"
   },
   "source": [
    "## <a name=\"loading_embeddings\">Loading embeddings</a>\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a7fd6571da10ea9502d6c2319cb938e826c6448a"
   },
   "outputs": [],
   "source": [
    "def load_embed(file):\n",
    "    \"\"\"Load embedding from file\n",
    "    \"\"\"\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "c66a2f1c2cc311da2d5a50b055a1eddfe5d7d9e7"
   },
   "outputs": [],
   "source": [
    "glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "487bdd123192f075cfc9bd5d64d0feeb74b8fccd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GloVe embedding\n",
      "Extracting Paragram embedding\n",
      "Extracting FastText embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting GloVe embedding\")\n",
    "embed_glove = load_embed(glove)\n",
    "print(\"Extracting Paragram embedding\")\n",
    "embed_paragram = load_embed(paragram)\n",
    "print(\"Extracting FastText embedding\")\n",
    "embed_fasttext = load_embed(wiki_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58cd4340154d888308002d25392830606a4df601"
   },
   "source": [
    "## <a name=\"vocabulary\">Vocabulary</a>\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "52eab6198a82b236da639211effc34a69e294d3d"
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    \"\"\"Build vocabulary with word as key and this words count as value\n",
    "    \"\"\"\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e107a3b6846a73386ac4f5ab8c3d3bdf240898e4"
   },
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    \"\"\"Check coverage of vocabulary in embedding\n",
    "    \"\"\"\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "aea82a77c480ee09181725be54732b4d1ec2df6d"
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c07e07632f9b6c2d0865e2e23b1b846ad3fc4a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 32.77% of vocab\n",
      "Found embeddings for  88.15% of all text\n",
      "Paragram : \n",
      "Found embeddings for 19.37% of vocab\n",
      "Found embeddings for  72.21% of all text\n",
      "FastText : \n",
      "Found embeddings for 29.77% of vocab\n",
      "Found embeddings for  87.66% of all text\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Paragram : \")\n",
    "oov_paragram = check_coverage(vocab, embed_paragram)\n",
    "print(\"FastText : \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5665a9363e2e6dc3c8891aad4ab5c849c6161783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 436013),\n",
       " ('I', 319441),\n",
       " ('How', 273144),\n",
       " ('Why', 148582),\n",
       " ('Is', 113627),\n",
       " ('Can', 54992),\n",
       " ('Which', 49357),\n",
       " ('Do', 41756),\n",
       " ('If', 35896),\n",
       " ('Are', 30442),\n",
       " ('Does', 24142),\n",
       " ('Who', 22884),\n",
       " ('Where', 20008),\n",
       " ('Should', 17269),\n",
       " ('India?', 17082),\n",
       " ('Will', 15283),\n",
       " ('When', 15084),\n",
       " ('India', 14270),\n",
       " ('Indian', 13441),\n",
       " ('it?', 13436),\n",
       " (\"I'm\", 13344),\n",
       " (\"What's\", 12985),\n",
       " ('Trump', 10569),\n",
       " ('Quora', 10447),\n",
       " ('In', 10441),\n",
       " ('Would', 10307),\n",
       " ('US', 9832),\n",
       " ('do?', 9112),\n",
       " ('My', 8463),\n",
       " ('The', 8215),\n",
       " ('life?', 8074),\n",
       " ('Did', 7891),\n",
       " ('Have', 7519),\n",
       " ('American', 6965),\n",
       " ('you?', 6553),\n",
       " ('me?', 6485),\n",
       " ('them?', 6421),\n",
       " ('Has', 6092),\n",
       " ('time?', 5994),\n",
       " ('world?', 5632),\n",
       " ('Chinese', 5629),\n",
       " ('English', 5594),\n",
       " ('people?', 5191),\n",
       " ('why?', 5144),\n",
       " ('Quora?', 4872),\n",
       " ('As', 4814),\n",
       " ('Could', 4697),\n",
       " ('Google', 4679),\n",
       " ('like?', 4677),\n",
       " ('China', 4648),\n",
       " ('for?', 4631),\n",
       " ('A', 4621),\n",
       " ('United', 4600),\n",
       " ('Americans', 4557),\n",
       " ('Was', 4534),\n",
       " ('JEE', 4431),\n",
       " ('work?', 4392),\n",
       " ('2017?', 4227),\n",
       " ('Muslims', 4172),\n",
       " ('mean?', 4137),\n",
       " ('North', 3850),\n",
       " ('TV', 3843),\n",
       " ('Donald', 3786),\n",
       " ('Indians', 3766),\n",
       " ('2018?', 3746),\n",
       " ('America', 3687),\n",
       " ('New', 3665),\n",
       " ('And', 3594),\n",
       " ('Facebook', 3593),\n",
       " ('country?', 3578),\n",
       " ('University', 3559),\n",
       " ('now?', 3496),\n",
       " ('this?', 3464),\n",
       " ('South', 3426),\n",
       " ('years?', 3387),\n",
       " ('Muslim', 3383),\n",
       " ('USA', 3338),\n",
       " ('not?', 3246),\n",
       " ('IIT', 3183),\n",
       " ('Android', 3087),\n",
       " ('British', 2990),\n",
       " ('MBA', 2932),\n",
       " ('year?', 2913),\n",
       " (\"I've\", 2893),\n",
       " ('It', 2834),\n",
       " ('day?', 2834),\n",
       " ('engineering?', 2743),\n",
       " ('person?', 2728),\n",
       " ('school?', 2688),\n",
       " ('so,', 2679),\n",
       " ('Canada', 2674),\n",
       " ('IT', 2664),\n",
       " ('YouTube', 2635),\n",
       " ('God', 2620),\n",
       " ('I’m', 2598),\n",
       " ('money?', 2559),\n",
       " ('system?', 2553),\n",
       " ('UK', 2547),\n",
       " (\"you've\", 2530),\n",
       " ('President', 2513),\n",
       " ('For', 2508),\n",
       " ('Why?', 2508),\n",
       " ('today?', 2500),\n",
       " ('be?', 2496),\n",
       " ('China?', 2471),\n",
       " ('job?', 2429),\n",
       " ('business?', 2387),\n",
       " (\"isn't\", 2366),\n",
       " ('Japanese', 2359),\n",
       " ('\"The', 2330),\n",
       " ('company?', 2330),\n",
       " ('Pakistan', 2326),\n",
       " ('States', 2303),\n",
       " ('online?', 2286),\n",
       " ('exam?', 2278),\n",
       " ('him?', 2275),\n",
       " ('Delhi', 2221),\n",
       " ('US?', 2198),\n",
       " ('don’t', 2175),\n",
       " ('Canada?', 2152),\n",
       " ('countries?', 2141),\n",
       " ('European', 2128),\n",
       " ('iPhone', 2098),\n",
       " ('out?', 2095),\n",
       " ('that?', 2088),\n",
       " ('Am', 2071),\n",
       " ('Russia', 2065),\n",
       " ('future?', 2046),\n",
       " ('women?', 2045),\n",
       " ('Or', 2039),\n",
       " ('language?', 2005),\n",
       " ('have?', 2001),\n",
       " ('sex?', 2000),\n",
       " ('account?', 1987),\n",
       " ('Modi', 1970),\n",
       " ('World', 1940),\n",
       " ('CSE', 1928),\n",
       " ('U.S.', 1923),\n",
       " ('Amazon', 1919),\n",
       " ('one?', 1915),\n",
       " ('America?', 1910),\n",
       " ('college?', 1908),\n",
       " ('up?', 1904),\n",
       " ('USA?', 1889),\n",
       " ('PhD', 1883),\n",
       " ('in?', 1864),\n",
       " ('He', 1862),\n",
       " ('Germany', 1853),\n",
       " ('home?', 1853),\n",
       " ('Russian', 1848),\n",
       " ('about?', 1842),\n",
       " ('After', 1840),\n",
       " ('Jews', 1837),\n",
       " ('there?', 1831),\n",
       " ('German', 1824),\n",
       " (\"aren't\", 1813),\n",
       " ('to?', 1787),\n",
       " ('days?', 1782),\n",
       " ('UPSC', 1774),\n",
       " ('French', 1770),\n",
       " ('What’s', 1751),\n",
       " ('MS', 1748),\n",
       " ('Apple', 1742),\n",
       " ('Korea', 1736),\n",
       " ('on?', 1734),\n",
       " ('Obama', 1729),\n",
       " ('English?', 1726),\n",
       " ('Bangalore?', 1723),\n",
       " ('history?', 1718),\n",
       " ('men?', 1718),\n",
       " (\"won't\", 1718),\n",
       " ('water?', 1712),\n",
       " ('We', 1710),\n",
       " ('Asian', 1691),\n",
       " ('Europe', 1691),\n",
       " ('relationship?', 1688),\n",
       " ('her?', 1686),\n",
       " ('students?', 1684),\n",
       " ('student?', 1679),\n",
       " ('Instagram', 1678),\n",
       " ('experience?', 1668),\n",
       " ('African', 1657),\n",
       " ('CBSE', 1632),\n",
       " ('Delhi?', 1624),\n",
       " (\"Trump's\", 1621),\n",
       " ('again?', 1618),\n",
       " ('science?', 1617),\n",
       " ('Israel', 1609),\n",
       " ('University?', 1605),\n",
       " ('of?', 1589),\n",
       " ('Islam', 1585),\n",
       " ('other?', 1577),\n",
       " ('Christians', 1572),\n",
       " ('website?', 1571),\n",
       " ('good?', 1562),\n",
       " ('market?', 1559),\n",
       " ('Hindu', 1555),\n",
       " ('Mains', 1536),\n",
       " ('Christian', 1533),\n",
       " ('phone?', 1526),\n",
       " ('love?', 1515),\n",
       " ('NEET', 1494),\n",
       " ('IQ', 1488),\n",
       " ('John', 1488),\n",
       " ('War', 1481),\n",
       " ('much?', 1480),\n",
       " ('At', 1477),\n",
       " ('Pakistan?', 1474),\n",
       " ('With', 1471),\n",
       " ('You', 1468),\n",
       " ('industry?', 1465),\n",
       " ('Trump?', 1462),\n",
       " ('Tamil', 1451),\n",
       " ('NIT', 1442),\n",
       " ('way?', 1439),\n",
       " ('from?', 1433),\n",
       " ('others?', 1408),\n",
       " ('SSC', 1401),\n",
       " ('society?', 1397),\n",
       " ('Mumbai?', 1396),\n",
       " ('number?', 1395),\n",
       " ('use?', 1394),\n",
       " ('To', 1390),\n",
       " ('Java', 1385),\n",
       " ('Earth', 1380),\n",
       " ('back?', 1377),\n",
       " ('with?', 1373),\n",
       " ('friends?', 1372),\n",
       " ('Any', 1371),\n",
       " ('me,', 1369),\n",
       " ('Harry', 1366),\n",
       " ('Australia', 1361),\n",
       " ('States?', 1358),\n",
       " ('career?', 1353),\n",
       " ('Hindi', 1351),\n",
       " ('Democrats', 1345),\n",
       " ('Star', 1340),\n",
       " ('Jesus', 1339),\n",
       " ('They', 1338),\n",
       " ('Australia?', 1336),\n",
       " ('CS', 1335),\n",
       " ('CA', 1334),\n",
       " ('better?', 1317),\n",
       " ('Mumbai', 1314),\n",
       " ('BJP', 1309),\n",
       " ('Europe?', 1308),\n",
       " ('UK?', 1305),\n",
       " ('Korean', 1296),\n",
       " ('app?', 1291),\n",
       " ('engineer?', 1291),\n",
       " ('exist?', 1290),\n",
       " ('West', 1289),\n",
       " ('Were', 1288),\n",
       " (\"Isn't\", 1288),\n",
       " ('name?', 1287),\n",
       " ('children?', 1286),\n",
       " ('Japan', 1282),\n",
       " ('Hindus', 1271),\n",
       " ('From', 1266),\n",
       " ('Western', 1264),\n",
       " ('body?', 1253),\n",
       " ('free?', 1252),\n",
       " ('questions?', 1239),\n",
       " ('made?', 1234),\n",
       " ('There', 1233),\n",
       " ('Since', 1233),\n",
       " ('girl?', 1233),\n",
       " ('war?', 1227),\n",
       " ('Windows', 1224),\n",
       " ('bad?', 1223),\n",
       " ('Canadian', 1221),\n",
       " (\"they're\", 1212),\n",
       " ('This', 1198),\n",
       " ('Hyderabad?', 1183),\n",
       " ('wrong?', 1183),\n",
       " ('MBBS', 1179),\n",
       " ('movies?', 1171),\n",
       " ('(or', 1170),\n",
       " ('Hitler', 1169),\n",
       " ('Jewish', 1166),\n",
       " ('possible?', 1164),\n",
       " ('course?', 1157),\n",
       " ('Germany?', 1156),\n",
       " ('WhatsApp', 1138),\n",
       " ('Hillary', 1136),\n",
       " ('development?', 1134),\n",
       " ('IAS', 1133),\n",
       " (\"haven't\", 1131),\n",
       " ('She', 1127),\n",
       " ('But', 1124),\n",
       " ('Black', 1119),\n",
       " ('die?', 1116),\n",
       " ('is?', 1114),\n",
       " ('East', 1107),\n",
       " ('White', 1105),\n",
       " ('Engineering', 1101),\n",
       " ('Bangalore', 1098),\n",
       " ('yes,', 1096),\n",
       " ('months?', 1092),\n",
       " ('(in', 1090),\n",
       " ('family?', 1087),\n",
       " ('know?', 1086),\n",
       " ('month?', 1081),\n",
       " ('San', 1078),\n",
       " ('Facebook?', 1078),\n",
       " ('culture?', 1077),\n",
       " ('On', 1076),\n",
       " ('Science', 1074),\n",
       " ('So', 1073),\n",
       " ('music?', 1072),\n",
       " ('state?', 1069),\n",
       " ('GATE', 1069),\n",
       " ('marriage?', 1066),\n",
       " ('house?', 1057),\n",
       " ('girls?', 1057),\n",
       " ('food?', 1056),\n",
       " ('degree?', 1056),\n",
       " ('friend?', 1055),\n",
       " ('test?', 1054),\n",
       " ('card?', 1050),\n",
       " ('Computer', 1049),\n",
       " ('someone?', 1044),\n",
       " ('Lord', 1039),\n",
       " ('can’t', 1037),\n",
       " ('books?', 1029),\n",
       " ('problem?', 1026),\n",
       " ('death?', 1023),\n",
       " ('university?', 1018),\n",
       " ('well?', 1017),\n",
       " ('Pune?', 1015),\n",
       " ('(I', 1012),\n",
       " ('interview?', 1011),\n",
       " ('government?', 1008),\n",
       " ('us?', 1006),\n",
       " ('program?', 1005),\n",
       " ('Samsung', 1002),\n",
       " ('car?', 998),\n",
       " ('Korea?', 998),\n",
       " ('India,', 997),\n",
       " ('man?', 996),\n",
       " ('thing?', 993),\n",
       " ('field?', 991),\n",
       " (\"what's\", 991),\n",
       " ('true?', 990),\n",
       " ('parents?', 986),\n",
       " ('exams?', 982),\n",
       " ('movie?', 978),\n",
       " ('child?', 977),\n",
       " ('Now', 977),\n",
       " (\"It's\", 975),\n",
       " ('religion?', 973),\n",
       " ('Microsoft', 973),\n",
       " ('One', 971),\n",
       " ('PC', 966),\n",
       " ('Japan?', 966),\n",
       " ('Spanish', 962),\n",
       " ('AI', 962),\n",
       " ('York', 962),\n",
       " ('place?', 959),\n",
       " ('GST', 957),\n",
       " ('win,', 955),\n",
       " ('power?', 955),\n",
       " ('all?', 954),\n",
       " ('night?', 954),\n",
       " (\"he's\", 953),\n",
       " ('game?', 952),\n",
       " ('same?', 952),\n",
       " ('media?', 950),\n",
       " ('else?', 950),\n",
       " ('happen?', 949),\n",
       " ('Saudi', 947),\n",
       " ('important?', 942),\n",
       " ('space?', 938),\n",
       " ('Pakistani', 934),\n",
       " ('marketing?', 931),\n",
       " ('National', 929),\n",
       " ('Russia?', 929),\n",
       " ('education?', 929),\n",
       " ('Africa?', 925),\n",
       " ('Africa', 925),\n",
       " ('technology?', 924),\n",
       " ('jobs?', 924),\n",
       " ('old?', 923),\n",
       " ('Bible', 921),\n",
       " ('so?', 921),\n",
       " ('(not', 921),\n",
       " ('period?', 917),\n",
       " ('life,', 917),\n",
       " ('State', 914),\n",
       " ('Islamic', 913),\n",
       " ('together?', 912),\n",
       " ('Middle', 911),\n",
       " ('Chennai?', 911),\n",
       " ('pregnant?', 909),\n",
       " ('it’s', 904),\n",
       " ('class?', 900),\n",
       " ('B', 900),\n",
       " ('depression?', 899),\n",
       " ('companies?', 898),\n",
       " ('James', 898),\n",
       " ('how?', 896),\n",
       " ('time,', 895),\n",
       " ('people,', 894),\n",
       " ('process?', 893),\n",
       " ('problems?', 891),\n",
       " ('better,', 891),\n",
       " ('things?', 891),\n",
       " ('DC', 890),\n",
       " ('skills?', 889),\n",
       " ('normal?', 889),\n",
       " ('College', 886),\n",
       " ('age?', 886),\n",
       " ('myself?', 883),\n",
       " ('woman?', 882),\n",
       " ('change?', 881),\n",
       " ('down?', 874),\n",
       " ('iOS', 874),\n",
       " ('Michael', 873),\n",
       " ('themselves?', 869),\n",
       " ('Clinton', 868),\n",
       " ('California', 868),\n",
       " ('2?', 864),\n",
       " ('ECE', 863),\n",
       " ('hair?', 860),\n",
       " ('God?', 858),\n",
       " ('C', 858),\n",
       " ('off?', 858),\n",
       " ('weight?', 857),\n",
       " ('done?', 854),\n",
       " ('yourself?', 852),\n",
       " ('energy?', 849),\n",
       " ('most?', 848),\n",
       " ('Republicans', 848),\n",
       " ('start?', 847),\n",
       " ('EU', 846),\n",
       " ('physics?', 846),\n",
       " ('SBI', 845),\n",
       " ('accomplishments?', 844),\n",
       " ('Roman', 835),\n",
       " ('France', 833),\n",
       " ('War?', 831),\n",
       " ('story?', 830),\n",
       " ('YouTube?', 829),\n",
       " ('not,', 828),\n",
       " ('Europeans', 828),\n",
       " ('law?', 825),\n",
       " ('right?', 820),\n",
       " ('Greek', 818),\n",
       " ('skin?', 818),\n",
       " (\"today's\", 817),\n",
       " ('CAT', 817),\n",
       " ('book?', 816),\n",
       " ('services?', 815),\n",
       " ('Singapore', 810),\n",
       " ('games?', 809),\n",
       " ('Pune', 809),\n",
       " (\"someone's\", 809),\n",
       " ('Iran', 806),\n",
       " ('humans?', 805),\n",
       " ('preparation?', 804),\n",
       " ('management?', 803),\n",
       " ('economy?', 799),\n",
       " ('face?', 798),\n",
       " ('DNA', 798),\n",
       " ('more?', 796),\n",
       " ('kids?', 794),\n",
       " ('VIT', 792),\n",
       " ('universe?', 791),\n",
       " ('used?', 789),\n",
       " ('Australian', 789),\n",
       " ('doesn’t', 787),\n",
       " ('week?', 787),\n",
       " ('Islam?', 786),\n",
       " ('learning?', 785),\n",
       " ('question?', 782),\n",
       " ('computer?', 779),\n",
       " ('date?', 777),\n",
       " ('series?', 777),\n",
       " ('cost?', 777),\n",
       " ('Whats', 775),\n",
       " ('(like', 775),\n",
       " ('earth?', 773),\n",
       " ('Bollywood', 769),\n",
       " ('Great', 764),\n",
       " ('live?', 763),\n",
       " ('George', 763),\n",
       " ('Civil', 762),\n",
       " ('style?', 762),\n",
       " ('Bitcoin', 759),\n",
       " ('player?', 753),\n",
       " ('times?', 752),\n",
       " ('real?', 751),\n",
       " ('service?', 749),\n",
       " ('School', 748),\n",
       " ('too?', 748),\n",
       " ('AIIMS', 748),\n",
       " ('Muslims?', 746),\n",
       " ('Potter', 746),\n",
       " ('Turkish', 745),\n",
       " ('abroad?', 741),\n",
       " ('Uber', 741),\n",
       " ('Hyderabad', 741),\n",
       " ('Instagram?', 740),\n",
       " ('3D', 738),\n",
       " ('London', 737),\n",
       " ('programming?', 733),\n",
       " ('Twitter', 730),\n",
       " ('Singapore?', 727),\n",
       " ('study?', 727),\n",
       " ('IIM', 725),\n",
       " ('BA', 724),\n",
       " ('pregnancy?', 724),\n",
       " ('mind?', 723),\n",
       " ('BITS', 720),\n",
       " ('etc?', 720),\n",
       " ('site?', 719),\n",
       " ('form?', 718),\n",
       " ('Quorans', 718),\n",
       " ('PG', 717),\n",
       " ('England', 716),\n",
       " ('level?', 715),\n",
       " ('Gandhi', 714),\n",
       " ('(and', 714),\n",
       " (\"one's\", 714),\n",
       " ('Big', 714),\n",
       " ('People', 712),\n",
       " ('health?', 710),\n",
       " ('politics?', 710),\n",
       " ('International', 710),\n",
       " ('ever?', 709),\n",
       " ('light?', 707),\n",
       " ('Please', 707),\n",
       " ('products?', 707),\n",
       " ('Bill', 705),\n",
       " ('Robert', 705),\n",
       " ('Kim', 705),\n",
       " ('anymore?', 702),\n",
       " ('disorder?', 700),\n",
       " ('project?', 698),\n",
       " ('pain?', 698),\n",
       " ('them,', 697),\n",
       " ('paper?', 697),\n",
       " ('away?', 696),\n",
       " ('read?', 696),\n",
       " ('area?', 693),\n",
       " ('CGL', 693),\n",
       " ('PM', 691),\n",
       " ('\"I', 691),\n",
       " (\"hasn't\", 691),\n",
       " ('California?', 691),\n",
       " ('internet?', 689),\n",
       " ('answers?', 687),\n",
       " ('Hollywood', 687),\n",
       " ('biography?', 686),\n",
       " (\"Don't\", 686),\n",
       " ('Im', 685),\n",
       " ('alone?', 683),\n",
       " ('own?', 683),\n",
       " (\"India's\", 682),\n",
       " ('go?', 682),\n",
       " ('Britain', 682),\n",
       " ('application?', 682),\n",
       " ('Wars', 680),\n",
       " ('first?', 675),\n",
       " ('idea?', 675),\n",
       " (\"shouldn't\", 674),\n",
       " ('Linux', 673),\n",
       " ('help?', 670),\n",
       " ('software?', 669),\n",
       " ('design?', 669),\n",
       " (\"people's\", 668),\n",
       " ('Israel?', 667),\n",
       " ('Marvel', 666),\n",
       " ('theory?', 666),\n",
       " ('something?', 666),\n",
       " ('Chennai', 665),\n",
       " ('Vietnam', 663),\n",
       " ('Arab', 663),\n",
       " ('SAT', 662),\n",
       " ('Institute', 662),\n",
       " ('anything?', 661),\n",
       " ('Kolkata?', 660),\n",
       " ('MBA?', 658),\n",
       " ('developer?', 658),\n",
       " ('Business', 656),\n",
       " ('Jio', 656),\n",
       " ('girlfriend?', 654),\n",
       " ('works?', 654),\n",
       " ('surgery?', 653),\n",
       " ('studies?', 653),\n",
       " ('Data', 653),\n",
       " ('Google?', 652),\n",
       " ('serve?', 651),\n",
       " ('data?', 650),\n",
       " ('MIT', 645),\n",
       " ('before?', 643),\n",
       " ('years,', 643),\n",
       " ('Java?', 640),\n",
       " ('Soviet', 640),\n",
       " ('Indians?', 639),\n",
       " ('animals?', 639),\n",
       " ('AC', 637),\n",
       " ('SEO', 636),\n",
       " (\"wasn't\", 636),\n",
       " ('David', 636),\n",
       " ('Russians', 633),\n",
       " ('OBC', 633),\n",
       " ('Given', 633),\n",
       " ('actor?', 632),\n",
       " ('reason?', 630),\n",
       " ('guy?', 630),\n",
       " ('Sri', 629),\n",
       " ('100%', 628),\n",
       " ('All', 626),\n",
       " ('past?', 626),\n",
       " ('GPA', 625),\n",
       " ('Christianity', 623),\n",
       " ('are?', 623),\n",
       " ('get?', 622),\n",
       " (\"there's\", 622),\n",
       " ('Congress', 621),\n",
       " (\"wouldn't\", 620),\n",
       " ('environment?', 620),\n",
       " ('League', 618),\n",
       " ('Galaxy', 618),\n",
       " ('King', 617),\n",
       " ('Air', 617),\n",
       " ('Quora,', 617),\n",
       " ('model?', 616),\n",
       " ('Bank', 615),\n",
       " ('answer?', 615),\n",
       " ('sleep?', 615),\n",
       " (\"couldn't\", 615),\n",
       " ('Turkey', 615),\n",
       " ('chemistry?', 614),\n",
       " ('visa?', 613),\n",
       " ('Dubai?', 611),\n",
       " ('Android?', 610),\n",
       " ('office?', 610),\n",
       " ('writing?', 609),\n",
       " ('Narendra', 608),\n",
       " ('team?', 606),\n",
       " ('Paul', 606),\n",
       " ('best?', 606),\n",
       " ('ago?', 605),\n",
       " ('Army', 605),\n",
       " ('Earth?', 604),\n",
       " ('Italian', 604),\n",
       " ('Nazi', 601),\n",
       " ('Mexico', 601),\n",
       " ('Republican', 601),\n",
       " ('gay?', 600),\n",
       " ('here?', 600),\n",
       " ('cancer?', 600),\n",
       " ('languages?', 598),\n",
       " ('eyes?', 598),\n",
       " ('officer?', 597),\n",
       " ('research?', 595),\n",
       " ('I’ve', 595),\n",
       " ('Elon', 592),\n",
       " ('dog?', 591),\n",
       " ('Americans?', 591),\n",
       " ('X', 591),\n",
       " ('world,', 587),\n",
       " ('words?', 587),\n",
       " ('training?', 587),\n",
       " ('now,', 587),\n",
       " ('ones?', 587),\n",
       " ('century?', 586),\n",
       " ('(for', 586),\n",
       " ('improved?', 585),\n",
       " ('brain?', 585),\n",
       " ('SAP', 585),\n",
       " ('issues?', 585),\n",
       " ('intelligence?', 585),\n",
       " ('10?', 581),\n",
       " ('House', 581),\n",
       " ('Snapchat', 579),\n",
       " ('WordPress', 577),\n",
       " ('Manipal', 576),\n",
       " ('president?', 575),\n",
       " ('US,', 574),\n",
       " ('function?', 573),\n",
       " ('HR', 573),\n",
       " ('city?', 572),\n",
       " ('Hindi?', 572),\n",
       " ('marks?', 571),\n",
       " (\"she's\", 570),\n",
       " ('dogs?', 569),\n",
       " ('May', 568),\n",
       " ('election?', 567),\n",
       " ('Management', 565),\n",
       " ('AP', 564),\n",
       " ('GRE', 562),\n",
       " ('public?', 561),\n",
       " ('happy?', 561),\n",
       " ('end?', 560),\n",
       " ('party?', 560),\n",
       " ('Hong', 559),\n",
       " ('videos?', 558),\n",
       " ('SRM', 557),\n",
       " ('Israeli', 556),\n",
       " ('yet?', 556),\n",
       " ('ICSE', 556),\n",
       " ('country,', 555),\n",
       " ('laptop?', 553),\n",
       " ('correct?', 551),\n",
       " ('IIT?', 550),\n",
       " ('machine?', 549),\n",
       " ('Python', 548),\n",
       " ('group?', 547),\n",
       " ('Union', 547),\n",
       " ('Putin', 547),\n",
       " ('WW2?', 546),\n",
       " ('produced?', 545),\n",
       " ('etc.?', 544),\n",
       " ('Eastern', 542),\n",
       " ('code?', 542),\n",
       " ('CSE?', 542),\n",
       " ('Asians', 542),\n",
       " ('fast?', 541),\n",
       " ('1?', 537),\n",
       " ('Some', 537),\n",
       " ('Game', 537),\n",
       " ('Nigeria?', 536),\n",
       " ('Party', 535),\n",
       " ('differ?', 534),\n",
       " ('anxiety?', 534),\n",
       " ('Germans', 532),\n",
       " ('case?', 531),\n",
       " ('Tesla', 531),\n",
       " ('Christmas', 530),\n",
       " ('actress?', 528),\n",
       " ('How?', 527),\n",
       " ('suicide?', 527),\n",
       " ('eat?', 526),\n",
       " ('12?', 525),\n",
       " ('only?', 525),\n",
       " ('12th?', 523),\n",
       " ('Asia', 523),\n",
       " ('Mars', 523),\n",
       " ('investment?', 523),\n",
       " ('Kashmir', 522),\n",
       " ('product?', 522),\n",
       " ('schools?', 522),\n",
       " ('TV?', 522),\n",
       " ('matter?', 522),\n",
       " ('person,', 520),\n",
       " ('economics?', 520),\n",
       " ('Even', 520),\n",
       " ('everything?', 520),\n",
       " ('easily?', 518),\n",
       " ('(e.g.', 518),\n",
       " ('price?', 517),\n",
       " ('PO', 516),\n",
       " ('Karnataka', 516),\n",
       " ('situation?', 516),\n",
       " ('General', 515),\n",
       " ('Christ', 513),\n",
       " ('William', 513),\n",
       " ('famous?', 512),\n",
       " ('psychology?', 512),\n",
       " ('City', 511),\n",
       " ('you’ve', 511),\n",
       " ('take?', 511),\n",
       " ('iPhone?', 507),\n",
       " ('Dubai', 505),\n",
       " ('ideas?', 505),\n",
       " ('PR', 504),\n",
       " ('Harvard', 504),\n",
       " ('fat?', 504),\n",
       " ('everyday?', 503),\n",
       " ('racist?', 503),\n",
       " ('startup?', 501),\n",
       " ('seen?', 501),\n",
       " ('Disney', 501),\n",
       " ('hours?', 501),\n",
       " ('boyfriend?', 501),\n",
       " ('Of', 501),\n",
       " ('systems?', 501),\n",
       " ('Amazon?', 500),\n",
       " ('Latin', 500),\n",
       " ('bank?', 499),\n",
       " ('called?', 498),\n",
       " ('London?', 498),\n",
       " ('Italy', 497),\n",
       " ('successful?', 497),\n",
       " ('married?', 497),\n",
       " ('room?', 497),\n",
       " ('different?', 496),\n",
       " ('CGPA', 495),\n",
       " ('No', 495),\n",
       " ('Mark', 494),\n",
       " ('Philippines?', 494),\n",
       " ('mathematics?', 493),\n",
       " ('created?', 492),\n",
       " ('blog?', 491),\n",
       " ('partner?', 491),\n",
       " ('Like', 491),\n",
       " ('summer?', 490),\n",
       " ('democracy?', 490),\n",
       " ('guys?', 490),\n",
       " (\"who's\", 489),\n",
       " ('growth?', 488),\n",
       " (\"world's\", 488),\n",
       " ('Batman', 487),\n",
       " ('JEE?', 486),\n",
       " ('Mains?', 485),\n",
       " ('Chinese?', 484),\n",
       " ('ISIS', 484),\n",
       " ('didn’t', 484),\n",
       " ('PC?', 484),\n",
       " ('CA?', 483),\n",
       " ('BBA', 483),\n",
       " ('Masters', 483),\n",
       " ('success?', 482),\n",
       " ('popular?', 482),\n",
       " ('do,', 481),\n",
       " ('lives?', 481),\n",
       " ('According', 480),\n",
       " ('nature?', 480),\n",
       " ('over?', 479),\n",
       " ('Steve', 478),\n",
       " ('Pakistanis', 478),\n",
       " ('numbers?', 478),\n",
       " ('teacher?', 477),\n",
       " ('nation?', 477),\n",
       " ('rate?', 477),\n",
       " ('choose?', 477),\n",
       " ('IP', 476),\n",
       " ('High', 475),\n",
       " ('Prime', 474),\n",
       " ('SC', 474),\n",
       " ('Gmail', 474),\n",
       " ('store?', 473),\n",
       " ('Kolkata', 471),\n",
       " ('Rahul', 470),\n",
       " ('reality?', 470),\n",
       " ('working?', 469),\n",
       " ('fight,', 469),\n",
       " ('line?', 469),\n",
       " ('loss?', 469),\n",
       " ('songs?', 469),\n",
       " ('sites?', 469),\n",
       " ('day,', 469),\n",
       " ('Empire', 468),\n",
       " ('examples?', 468),\n",
       " ('Central', 468),\n",
       " ('Los', 468),\n",
       " ('disease?', 468),\n",
       " ('issue?', 468),\n",
       " ('wife?', 467),\n",
       " ('Indonesia?', 467),\n",
       " ('France?', 467),\n",
       " ('learn?', 466),\n",
       " ('another?', 466),\n",
       " ('knowledge?', 466),\n",
       " ('head?', 466),\n",
       " ('race?', 465),\n",
       " ('Stephen', 465),\n",
       " ('community?', 465),\n",
       " (\"person's\", 465),\n",
       " ('Naruto', 465),\n",
       " ('Catholic', 464),\n",
       " ('Arabic', 464),\n",
       " ('show?', 464),\n",
       " ('mother?', 463),\n",
       " (\"women's\", 463),\n",
       " ('(the', 462),\n",
       " ('Kerala', 462),\n",
       " ('2019?', 462),\n",
       " ('personality?', 462),\n",
       " ('policy?', 462),\n",
       " ('say?', 461),\n",
       " ('make?', 460),\n",
       " ('Dr.', 460),\n",
       " ('(as', 460),\n",
       " ('itself?', 460),\n",
       " ('Asia?', 458),\n",
       " ('apps?', 458),\n",
       " ('Khan', 457),\n",
       " ('Indonesia', 457),\n",
       " ('PDF', 456),\n",
       " ('value?', 455),\n",
       " ('Mexican', 455),\n",
       " ('season?', 455),\n",
       " ('solution?', 455),\n",
       " ('engine?', 455),\n",
       " ('Being', 454),\n",
       " ('example,', 453),\n",
       " ('military?', 453),\n",
       " ('Arabs', 452),\n",
       " ('Texas', 449),\n",
       " ('Just', 449),\n",
       " ('IIIT', 447),\n",
       " ('Democratic', 447),\n",
       " ('Vietnamese', 447),\n",
       " ('Queen', 446),\n",
       " ('page?', 445),\n",
       " ('MSc', 445),\n",
       " ('safe?', 444),\n",
       " ('control?', 443),\n",
       " ('Allen', 442),\n",
       " ('art?', 441),\n",
       " ('Malaysia', 441),\n",
       " ('Palestinians', 441),\n",
       " ('video?', 441),\n",
       " ('school,', 440),\n",
       " ('\"the', 440),\n",
       " ('4?', 440),\n",
       " ('determined?', 439),\n",
       " ('category?', 439),\n",
       " ('Indonesian', 439),\n",
       " ('Physics', 438),\n",
       " ('FBI', 438),\n",
       " ('Native', 438),\n",
       " ('courses?', 437),\n",
       " ('Charles', 437),\n",
       " ('Gordon', 437),\n",
       " ('IS', 437),\n",
       " ('opinion,', 436),\n",
       " ('Spain', 436),\n",
       " ('Irish', 436),\n",
       " ('China,', 434),\n",
       " ('Red', 434),\n",
       " ('network?', 433),\n",
       " ('diet?', 433),\n",
       " ('sentence?', 432),\n",
       " ('think?', 432),\n",
       " ('graduation?', 432),\n",
       " ('states?', 431),\n",
       " ('plan?', 430),\n",
       " ('force?', 430),\n",
       " ('RBI', 429),\n",
       " ('score?', 429),\n",
       " ('Northern', 429),\n",
       " ('trading?', 428),\n",
       " ('Bible?', 428),\n",
       " ('doctor?', 428),\n",
       " ('Syria', 428),\n",
       " ('Prince', 428),\n",
       " ('NCERT', 427),\n",
       " ('established?', 426),\n",
       " ('Trump,', 426),\n",
       " ('oil?', 426),\n",
       " ('rights?', 425),\n",
       " ('moment?', 424),\n",
       " ('Mr.', 424),\n",
       " ('choice?', 424),\n",
       " ('Peter', 424),\n",
       " ('Internet', 424),\n",
       " ('general?', 424),\n",
       " (\"Shouldn't\", 424),\n",
       " ('Muhammad', 423),\n",
       " ('BITSAT', 423),\n",
       " ('Richard', 423),\n",
       " ('2016?', 423),\n",
       " ('nowadays?', 423),\n",
       " ('developed?', 423),\n",
       " ('TCS', 422),\n",
       " ('IELTS', 421),\n",
       " ('stupid?', 421),\n",
       " ('Thomas', 421),\n",
       " ('IPL', 421),\n",
       " ('Netflix', 421),\n",
       " ('buy?', 421),\n",
       " ('Washington', 421),\n",
       " ('year,', 418),\n",
       " ('etc.)', 418),\n",
       " ('Quran', 418),\n",
       " ('NDA', 417),\n",
       " ('3?', 417),\n",
       " ('NASA', 417),\n",
       " ('acid?', 417),\n",
       " ('relationships?', 417),\n",
       " ('OK', 417),\n",
       " ('enough?', 417),\n",
       " ('writer?', 417),\n",
       " ('Mexico?', 416),\n",
       " ('song?', 416),\n",
       " ('manufactured?', 416),\n",
       " ('cars?', 415),\n",
       " ('pressure?', 415),\n",
       " ('rank?', 415),\n",
       " ('communication?', 413),\n",
       " ('option?', 413),\n",
       " (\"China's\", 413),\n",
       " ('colleges?', 412),\n",
       " ('England?', 412),\n",
       " ('long?', 412),\n",
       " ('HP', 411),\n",
       " ('An', 411),\n",
       " ('analysis?', 411),\n",
       " ('Government', 410),\n",
       " ('population?', 410),\n",
       " ('Super', 410),\n",
       " ('UPSC?', 409),\n",
       " ('Nepal?', 409),\n",
       " ('salary?', 409),\n",
       " ('results?', 408),\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paragam seems to have a significantly lower coverage. Lets check why\n",
    "oov_paragram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "693c63ade211499a294310b0c3517ba8daf0b2d0"
   },
   "source": [
    "It does not understand upper letters, let us lower our texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "18d9e53372fd2cf2bf1191835793f2c23eee8686"
   },
   "outputs": [],
   "source": [
    "df['lowered_question'] = df['question_text'].apply(lambda x: x.lower())\n",
    "vocab_low = build_vocab(df['lowered_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fa3da2e41615b715a09bda6509f05716983dbe25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 27.10% of vocab\n",
      "Found embeddings for  87.88% of all text\n",
      "Paragram : \n",
      "Found embeddings for 31.01% of vocab\n",
      "Found embeddings for  88.21% of all text\n",
      "FastText : \n",
      "Found embeddings for 21.74% of vocab\n",
      "Found embeddings for  87.14% of all text\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab_low, embed_glove)\n",
    "print(\"Paragram : \")\n",
    "oov_paragram = check_coverage(vocab_low, embed_paragram)\n",
    "print(\"FastText : \")\n",
    "oov_fasttext = check_coverage(vocab_low, embed_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e14f66a02b363048fd911606e2d64a277eefd952"
   },
   "source": [
    "Good, but we lost a bit of information on the other embeddings.\n",
    "\n",
    "We have some words that are known with upper letters and uknown without.\n",
    "\n",
    "We will add lower word in embedding if it doesn't have an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "2d609e36ff047d7aca9484fbbecfee36492eeeb0"
   },
   "outputs": [],
   "source": [
    "def add_lower(embedding, vocab):\n",
    "    \"\"\"Add lower vocab words in embedding\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "3e4703a8326cea1e71c2f7ecbe8cf39e8dc6c257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Added 15199 words to embedding\n",
      "Paragram : \n",
      "Added 0 words to embedding\n",
      "FastText : \n",
      "Added 27908 words to embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove : \")\n",
    "add_lower(embed_glove, vocab)\n",
    "print(\"Paragram : \")\n",
    "add_lower(embed_paragram, vocab)\n",
    "print(\"FastText : \")\n",
    "add_lower(embed_fasttext, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c3d96c47287581bf4cb4f45568963da1c36c0b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 30.39% of vocab\n",
      "Found embeddings for  88.19% of all text\n",
      "Paragram : \n",
      "Found embeddings for 31.01% of vocab\n",
      "Found embeddings for  88.21% of all text\n",
      "FastText : \n",
      "Found embeddings for 27.77% of vocab\n",
      "Found embeddings for  87.73% of all text\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab_low, embed_glove)\n",
    "print(\"Paragram : \")\n",
    "oov_paragram = check_coverage(vocab_low, embed_paragram)\n",
    "print(\"FastText : \")\n",
    "oov_fasttext = check_coverage(vocab_low, embed_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "4c39feeec6a05ce73c5dbfe948a8ac39bc8432fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('india?', 17092),\n",
       " (\"what's\", 13977),\n",
       " ('it?', 13702),\n",
       " ('do?', 9125),\n",
       " ('life?', 8114),\n",
       " ('why?', 7674),\n",
       " ('you?', 6572),\n",
       " ('me?', 6525),\n",
       " ('them?', 6423),\n",
       " ('time?', 6021)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check out of vocabulary words\n",
    "oov_glove[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5157dcad820ad7fbfad38e9db9ea33dfc95f4a8b"
   },
   "source": [
    "Problems:\n",
    "- contractions\n",
    "- punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69c4c6d849b4d04aae8599943f420aba8f515d6a"
   },
   "source": [
    "### <a name=\"contractions\">Contractions</a>\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "27c8df60a14e61216c68b08dc16ce63a2fa9af58"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "a3de8783d6e20ee2b5017cfff32cb45615afc609"
   },
   "outputs": [],
   "source": [
    "def known_contractions(embed):\n",
    "    \"\"\"Checks contractions in embedding\n",
    "    \"\"\"\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "30a32ac3fe9bb96ce083ffc20b5424cf68cbc565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contractions in embeddings:\n",
      "   Glove :\n",
      "[\"can't\", \"'cause\", \"didn't\", \"doesn't\", \"don't\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"it's\", \"ma'am\", \"o'clock\", \"that's\", \"you'll\", \"you're\"]\n",
      "   Paragram :\n",
      "[\"can't\", \"'cause\", \"didn't\", \"doesn't\", \"don't\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"it's\", \"ma'am\", \"o'clock\", \"that's\", \"you'll\", \"you're\"]\n",
      "   FastText :\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"Contractions in embeddings:\")\n",
    "print(\"   Glove :\")\n",
    "print(known_contractions(embed_glove))\n",
    "print(\"   Paragram :\")\n",
    "print(known_contractions(embed_paragram))\n",
    "print(\"   FastText :\")\n",
    "print(known_contractions(embed_fasttext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3b1c754422f5a626f0ee614a8ff028b18a512f93"
   },
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    \"\"\"Remove contractions from text\n",
    "    \"\"\"\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "d48b94c13bb96024b2e9d07af65381ad8822bae6"
   },
   "outputs": [],
   "source": [
    "df['treated_question'] = df['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "f10a637f271b5fa8e5686ee605094e561eb9d2ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 30.53% of vocab\n",
      "Found embeddings for  88.56% of all text\n",
      "Paragram : \n",
      "Found embeddings for 31.16% of vocab\n",
      "Found embeddings for  88.58% of all text\n",
      "FastText : \n",
      "Found embeddings for 27.91% of vocab\n",
      "Found embeddings for  88.44% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df['treated_question'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Paragram : \")\n",
    "oov_paragram = check_coverage(vocab, embed_paragram)\n",
    "print(\"FastText : \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7e8e3eb91ba19498be755b7fc0c9f72ecffe3937"
   },
   "source": [
    "### <a name=\"special_characters\">Special characters</a>\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "3d521079f9df744e0e52eecde6e91233068bf9be"
   },
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "49a40543a29cf80e34cdd9149715bb39576d8c76"
   },
   "outputs": [],
   "source": [
    "def unknown_punct(embed, punct):\n",
    "    \"\"\"Find uknown punctuations in embedding\n",
    "    \"\"\"\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "9267d670de46fafad994617d8930ee24117d911e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove :\n",
      "“ ” ’ ∞ θ ÷ α • à − β ∅ ³ π ‘ ₹ ´ ° £ € × ™ √ ² — – \n",
      "Paragram :\n",
      "“ ” ’ ∞ θ ÷ α • à − β ∅ ³ π ‘ ₹ ´ ° £ € × ™ √ ² — – \n",
      "FastText :\n",
      "_ ` \n"
     ]
    }
   ],
   "source": [
    "print(\"Glove :\")\n",
    "print(unknown_punct(embed_glove, punct))\n",
    "print(\"Paragram :\")\n",
    "print(unknown_punct(embed_paragram, punct))\n",
    "print(\"FastText :\")\n",
    "print(unknown_punct(embed_fasttext, punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "f16752f126043e0bd310f3a6621452c7801d8ca1"
   },
   "outputs": [],
   "source": [
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "c204be5d971e3e5ce501193e4efbcc3347283f2a"
   },
   "outputs": [],
   "source": [
    "def clean_special_chars(text, punct, mapping):\n",
    "    \"\"\"Remove special characters from text\n",
    "    \"\"\"\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "e49d242acab77ea221b1cefb0e7b40179ec9c913"
   },
   "outputs": [],
   "source": [
    "# Use a map to replace unknown characters with known ones.\n",
    "df['treated_question'] = df['treated_question'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "200ed0c5fd9b6d89df7f286cc260dab97ddc1914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 69.10% of vocab\n",
      "Found embeddings for  99.58% of all text\n",
      "Paragram : \n",
      "Found embeddings for 73.58% of vocab\n",
      "Found embeddings for  99.63% of all text\n",
      "FastText : \n",
      "Found embeddings for 60.75% of vocab\n",
      "Found embeddings for  99.45% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df['treated_question'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Paragram : \")\n",
    "oov_paragram = check_coverage(vocab, embed_paragram)\n",
    "print(\"FastText : \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "1091f9e60e60f88b2b705f305a87d773c66c66cb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quorans', 885),\n",
       " ('bitsat', 583),\n",
       " ('kvpy', 369),\n",
       " ('comedk', 369),\n",
       " ('quoran', 325),\n",
       " ('wbjee', 246),\n",
       " ('articleship', 218),\n",
       " ('viteee', 193),\n",
       " ('fortnite', 166),\n",
       " ('upes', 164),\n",
       " ('marksheet', 151),\n",
       " ('afcat', 131),\n",
       " ('uceed', 126),\n",
       " ('dropshipping', 123),\n",
       " ('bhakts', 118),\n",
       " ('iitjee', 114),\n",
       " ('machedo', 112),\n",
       " ('upsee', 111),\n",
       " ('bnbr', 105),\n",
       " ('alshamsi', 100),\n",
       " ('chsl', 100),\n",
       " ('iitian', 99),\n",
       " ('amcat', 97),\n",
       " ('josaa', 96),\n",
       " ('unacademy', 89),\n",
       " ('zerodha', 85),\n",
       " ('qoura', 85),\n",
       " ('nmat', 80),\n",
       " ('icos', 79),\n",
       " ('jiit', 78),\n",
       " ('hairfall', 73),\n",
       " ('lnmiit', 73),\n",
       " ('metoo', 71),\n",
       " ('kavalireddi', 71),\n",
       " ('doklam', 70),\n",
       " ('muoet', 68),\n",
       " ('woocommerce', 67),\n",
       " ('nicmar', 66),\n",
       " ('vajiram', 62),\n",
       " ('srmjee', 61),\n",
       " ('modiji', 61),\n",
       " ('infjs', 60),\n",
       " ('adhaar', 60),\n",
       " ('zebpay', 58),\n",
       " ('elitmus', 58),\n",
       " ('pubg', 57),\n",
       " ('awdhesh', 55),\n",
       " ('hackerrank', 54),\n",
       " ('gixxer', 54),\n",
       " ('aiq', 53),\n",
       " ('sibm', 53),\n",
       " ('koinex', 50),\n",
       " ('golang', 50),\n",
       " ('mahadasha', 49),\n",
       " ('mhcet', 47),\n",
       " ('byju', 47),\n",
       " ('binance', 46),\n",
       " ('srmjeee', 44),\n",
       " ('playstore', 44),\n",
       " ('mnit', 43),\n",
       " ('ftre', 42),\n",
       " ('skripal', 42),\n",
       " ('sgsits', 42),\n",
       " ('howdo', 41),\n",
       " ('mhtcet', 40),\n",
       " ('tatkal', 40),\n",
       " ('whatare', 39),\n",
       " ('bipc', 39),\n",
       " ('lbsnaa', 38),\n",
       " ('mastrubation', 38),\n",
       " ('pdpu', 38),\n",
       " ('bmsce', 38),\n",
       " ('jiofi', 38),\n",
       " ('nanodegree', 38),\n",
       " ('upse', 37),\n",
       " ('jbims', 37),\n",
       " ('sarahah', 36),\n",
       " ('tanx', 36),\n",
       " ('pcod', 35),\n",
       " ('daiict', 35),\n",
       " ('youtu', 34),\n",
       " ('usict', 34),\n",
       " ('reactjs', 34),\n",
       " ('dushka', 34),\n",
       " ('aits', 33),\n",
       " ('nchmct', 33),\n",
       " ('iisers', 33),\n",
       " ('bittrex', 32),\n",
       " ('cdse', 31),\n",
       " ('pessat', 31),\n",
       " ('infps', 31),\n",
       " ('diestm', 31),\n",
       " ('codechef', 31),\n",
       " ('swiggy', 31),\n",
       " ('xxxtentacion', 30),\n",
       " ('passouts', 30),\n",
       " ('mppsc', 30),\n",
       " ('dream11', 30),\n",
       " ('demonitisation', 30),\n",
       " ('galgotia', 29)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_fasttext[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "621bc84a94efff46e41b0eeb182c91f0f9de77ad"
   },
   "source": [
    "### <a name=\"spelling_mistakes\">Spelling mistakes</a>\n",
    "\n",
    "Still missing:\n",
    "- Unknown words\n",
    "- Acronyms(скорочення, абрівеатура)\n",
    "- Spelling mistakes\n",
    "\n",
    "Lets try manually fix some spelling mistakes\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "a7479b37c0e05445a0d0090ed23703174ac7e13d"
   },
   "outputs": [],
   "source": [
    "# Most frequent mispells\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "9abb855ca161320c2808c85d2b591f642afb8f9e"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(x, dic):\n",
    "    \"\"\"Replace incorrect words in text from correct words dictionary\n",
    "    \"\"\"\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "03f572c372fe5c7aebae479b601a3f572063ec50"
   },
   "outputs": [],
   "source": [
    "df['treated_question'] = df['treated_question'].apply(lambda x: correct_spelling(x, mispell_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "bec6287a4a18dd47f36b13de893e9c4242630431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 69.09% of vocab\n",
      "Found embeddings for  99.58% of all text\n",
      "Paragram : \n",
      "Found embeddings for 73.58% of vocab\n",
      "Found embeddings for  99.63% of all text\n",
      "FastText : \n",
      "Found embeddings for 60.74% of vocab\n",
      "Found embeddings for  99.45% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df['treated_question'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)\n",
    "print(\"Paragram : \")\n",
    "oov_paragram = check_coverage(vocab, embed_paragram)\n",
    "print(\"FastText : \")\n",
    "oov_fasttext = check_coverage(vocab, embed_fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9cb4fdf24f02922aeff0a77dbf11f95a3589c06f"
   },
   "source": [
    "### <a name=\"simplenn\">Simple feed-forward neural network model</a>\n",
    "\n",
    "In this module I used simple nn from lecture materials.\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "3da8e5496ffed0d0eda0ed4e2b8502fa54ba3204"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk import word_tokenize\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "cc86fa62da5b747c3d7273bd7900a5701401a4eb"
   },
   "outputs": [],
   "source": [
    "random_state = random.getstate()\n",
    "batch_size = 64\n",
    "\n",
    "# init text field\n",
    "text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize)\n",
    "qid = torchtext.data.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "9843197ae7543b3bde57e71db9e0d30386f2d5e0"
   },
   "outputs": [],
   "source": [
    "target = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "48346dda16090c72e189f1278cf400e96618c89e"
   },
   "outputs": [],
   "source": [
    "# Read train dataset\n",
    "train_trch = torchtext.data.TabularDataset(path='../input/train.csv', format='csv',\n",
    "                                      fields={'question_text': ('text',text),\n",
    "                                              'target': ('target',target)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "b606d4f14f494a1b487d0e762cf728c39650e22d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999994 [00:00<?, ?it/s]Skipping token b'999994' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|█████████▉| 999750/999994 [02:50<00:00, 5541.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1957, 300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 999750/999994 [03:00<00:00, 5541.98it/s]"
     ]
    }
   ],
   "source": [
    "text.build_vocab(df['treated_question']) # build vocab from existing preprocessed one for particular embedding\n",
    "text.vocab.load_vectors(torchtext.vocab.Vectors('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'))\n",
    "print(text.vocab.vectors.shape)\n",
    "\n",
    "# split dataframe to train_trch(train) and val(test) datasets\n",
    "train_trch, val = train_trch.split(split_ratio=0.8, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "64fce56fe43dd1d9d57f8195e339bbe2e2769e48"
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, pretrained_lm, padding_idx, hidden_dim, static=True):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        # the first level of nn\n",
    "        # input: 300, output: 1\n",
    "        self.linear_1 = nn.Linear(300, 1)\n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        # first level training\n",
    "        x = self.linear_1(x)\n",
    "        # linear_11 = nn.Linear(100, 1).cuda()\n",
    "        # x = linear_11(x)\n",
    "        (a,b,c) = x.size()\n",
    "        # decrease dimension\n",
    "        x = x.view(a,b)\n",
    "        # second level in: b out: 1\n",
    "        linear_2 = nn.Linear(b, 1).cuda()\n",
    "        # second level training\n",
    "        x = linear_2(x)\n",
    "        # for debugging\n",
    "        # import pdb; pdb.set_trace()\n",
    "        \n",
    "        # in the end we have a size (a, 1)\n",
    "        return F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "54b6452c8bab4c5b32db9df363ff29cf17c4556b"
   },
   "outputs": [],
   "source": [
    "def training(epoch, model, loss_func, optimizer, train_iter, val_iter):\n",
    "    \"\"\"Train the simple model\n",
    "    input: \n",
    "    epoch - epoch number\n",
    "    model - simple model\n",
    "    loss_func - loss function\n",
    "    train_iter - iterator throgh train dataset\n",
    "    val_ter - iterator throgh test dataset\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    train_record = []\n",
    "    losses = []\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        # Set up the batch generator for a new epoch.\n",
    "        train_iter.init_epoch()\n",
    "        for train_batch in iter(train_iter):\n",
    "            step += 1\n",
    "            model.train()\n",
    "            # x - text, y - target\n",
    "            x = train_batch.text.cuda()\n",
    "            y = train_batch.target.type(torch.Tensor).cuda()\n",
    "            model.zero_grad()\n",
    "            # do prediction\n",
    "            pred = model.forward(x).view(-1)\n",
    "            # count loss\n",
    "            loss = loss_func(pred, y)\n",
    "            loss_data = loss.cpu().data.numpy()\n",
    "            train_record.append(loss_data)\n",
    "            # make a tip for nn with loss function\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print loss and step number for train\n",
    "            if step % 1000 == 0:\n",
    "                print(\"Step: {:06}, loss {:.4f}\".format(step, loss_data))\n",
    "            if step % 10000 == 0:\n",
    "                # pred and print the results for train and test dataset\n",
    "                model.eval()\n",
    "                model.zero_grad()\n",
    "                val_loss = []\n",
    "                for val_batch in iter(val_iter):\n",
    "                    val_x = val_batch.text.cuda()\n",
    "                    val_y = val_batch.target.type(torch.Tensor).cuda()\n",
    "                    val_pred = model.forward(val_x).view(-1)\n",
    "                    val_loss.append(loss_func(val_pred, val_y).cpu().data.numpy())\n",
    "                val_record = []\n",
    "                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n",
    "                print('Epoch {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(\n",
    "                            e, step, np.mean(train_record), val_record[-1]['loss']))\n",
    "                train_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "99cf00c4bf15bfb549b8fec6cca3615a923fdbda"
   },
   "outputs": [],
   "source": [
    "# Create SimpleModel\n",
    "model = SimpleModel(text.vocab.vectors,\n",
    "                    padding_idx=text.vocab.stoi[text.pad_token],\n",
    "                    hidden_dim=128).cuda()\n",
    "loss_function = nn.BCEWithLogitsLoss()# with 2 neurons loss -> cross entropy\n",
    "# Adam: A Method for Stochastic Optimization\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=1e-3)\n",
    "\n",
    "# Create an iterators that batches examples of similar lengths together.\n",
    "\n",
    "train_iter = torchtext.data.BucketIterator(dataset=train_trch,\n",
    "                                           batch_size=batch_size,\n",
    "                                           sort_key=lambda x: x.text.__len__(),\n",
    "                                           shuffle=True,\n",
    "                                           sort=False)\n",
    "\n",
    "val_iter = torchtext.data.BucketIterator(dataset=val,\n",
    "                                         batch_size=batch_size,\n",
    "                                         sort_key=lambda x: x.text.__len__(),\n",
    "                                         train=False,\n",
    "                                         sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "cf9f2f8c8c7b075507828e12b08b3404ef156f89",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 001000, loss 0.6931\n",
      "Step: 002000, loss 0.6931\n",
      "Step: 003000, loss 0.7250\n",
      "Step: 004000, loss 0.6931\n",
      "Step: 005000, loss 0.6932\n",
      "Step: 006000, loss 0.7359\n",
      "Step: 007000, loss 0.7060\n",
      "Step: 008000, loss 0.7005\n",
      "Step: 009000, loss 0.7221\n",
      "Step: 010000, loss 0.7740\n",
      "Epoch 00 - step 010000 - train_loss 0.7099 - val_loss 0.7112 \n",
      "Step: 011000, loss 0.6931\n",
      "Step: 012000, loss 0.7102\n",
      "Step: 013000, loss 0.6931\n",
      "Step: 014000, loss 0.6931\n",
      "Step: 015000, loss 0.7504\n",
      "Step: 016000, loss 0.6932\n",
      "Step: 017000, loss 0.6931\n",
      "Step: 018000, loss 0.6931\n",
      "Step: 019000, loss 0.6931\n",
      "Step: 020000, loss 0.6931\n",
      "Epoch 01 - step 020000 - train_loss 0.7105 - val_loss 0.7104 \n",
      "Step: 021000, loss 0.6931\n",
      "Step: 022000, loss 0.6931\n",
      "Step: 023000, loss 0.7658\n",
      "Step: 024000, loss 0.7173\n",
      "Step: 025000, loss 0.7264\n",
      "Step: 026000, loss 0.7689\n",
      "Step: 027000, loss 0.6931\n",
      "Step: 028000, loss 0.7217\n",
      "Step: 029000, loss 0.6931\n",
      "Step: 030000, loss 0.6931\n",
      "Epoch 01 - step 030000 - train_loss 0.7102 - val_loss 0.7106 \n",
      "Step: 031000, loss 0.7672\n",
      "Step: 032000, loss 0.7459\n",
      "Step: 033000, loss 0.6932\n",
      "Step: 034000, loss 0.6931\n",
      "Step: 035000, loss 0.6933\n",
      "Step: 036000, loss 0.7374\n",
      "Step: 037000, loss 0.6931\n",
      "Step: 038000, loss 0.7380\n",
      "Step: 039000, loss 0.6931\n",
      "Step: 040000, loss 0.7383\n",
      "Epoch 02 - step 040000 - train_loss 0.7105 - val_loss 0.7100 \n",
      "Step: 041000, loss 0.7215\n",
      "Step: 042000, loss 0.7362\n",
      "Step: 043000, loss 0.7020\n",
      "Step: 044000, loss 0.7345\n",
      "Step: 045000, loss 0.7527\n",
      "Step: 046000, loss 0.6935\n",
      "Step: 047000, loss 0.6931\n",
      "Step: 048000, loss 0.6931\n",
      "Step: 049000, loss 0.6931\n",
      "Step: 050000, loss 0.6931\n",
      "Epoch 03 - step 050000 - train_loss 0.7106 - val_loss 0.7099 \n",
      "Step: 051000, loss 0.7158\n",
      "Step: 052000, loss 0.7495\n",
      "Step: 053000, loss 0.7122\n",
      "Step: 054000, loss 0.6979\n",
      "Step: 055000, loss 0.7225\n",
      "Step: 056000, loss 0.6931\n",
      "Step: 057000, loss 0.7076\n",
      "Step: 058000, loss 0.7622\n",
      "Step: 059000, loss 0.7574\n",
      "Step: 060000, loss 0.7296\n",
      "Epoch 03 - step 060000 - train_loss 0.7107 - val_loss 0.7109 \n",
      "Step: 061000, loss 0.6931\n",
      "Step: 062000, loss 0.6931\n",
      "Step: 063000, loss 0.6931\n",
      "Step: 064000, loss 0.7284\n",
      "Step: 065000, loss 0.7294\n",
      "Step: 066000, loss 0.7298\n",
      "Step: 067000, loss 0.7520\n",
      "Step: 068000, loss 0.7104\n",
      "Step: 069000, loss 0.6931\n",
      "Step: 070000, loss 0.6931\n",
      "Epoch 04 - step 070000 - train_loss 0.7107 - val_loss 0.7097 \n",
      "Step: 071000, loss 0.6931\n",
      "Step: 072000, loss 0.7557\n",
      "Step: 073000, loss 0.7306\n",
      "Step: 074000, loss 0.6931\n",
      "Step: 075000, loss 0.6931\n",
      "Step: 076000, loss 0.7399\n",
      "Step: 077000, loss 0.6992\n",
      "Step: 078000, loss 0.6931\n",
      "Step: 079000, loss 0.6931\n",
      "Step: 080000, loss 0.6931\n",
      "Epoch 04 - step 080000 - train_loss 0.7107 - val_loss 0.7100 \n",
      "Step: 081000, loss 0.6957\n"
     ]
    }
   ],
   "source": [
    "training(model=model,\n",
    "         epoch=5,\n",
    "         loss_func=loss_function,\n",
    "         optimizer=optimizer,\n",
    "         train_iter=train_iter,\n",
    "         val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "ad6ce0d6d12f9eaec0595def4485a36da8c97cd3"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_pred = []\n",
    "val_true = []\n",
    "val_iter.init_epoch()\n",
    "for val_batch in iter(val_iter):\n",
    "    val_x = val_batch.text.cuda()\n",
    "    val_true += val_batch.target.data.numpy().tolist()\n",
    "    val_pred += torch.sigmoid(model.forward(val_x).view(-1)).cpu().data.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "351b74e8586ae19f74ea761a416fb1216af09e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.1000 with F1 score: 0.1152\n"
     ]
    }
   ],
   "source": [
    "tmp = [0,0,0] # idx, cur, max\n",
    "delta = 0\n",
    "for tmp[0] in np.arange(0.1, 0.501, 0.01):\n",
    "    tmp[1] = f1_score(val_true, np.array(val_pred)>tmp[0])\n",
    "    if tmp[1] > tmp[2]:\n",
    "        delta = tmp[0]\n",
    "        tmp[2] = tmp[1]\n",
    "print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3de750956c28fa6ef16a439b581c93d9739f08ec"
   },
   "source": [
    "### <a name=\"logit\">TF-IDF vectorizer and Logistic regression</a>\n",
    "\n",
    "In this module I used example with logistic regression and tf-idf vectorizer from kernel: https://www.kaggle.com/demery/character-level-tfidf-logistic-regression\n",
    "\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "a23666c5e4def60bb3d69d2b7234e11589312c94"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import f1_score\n",
    "import gc\n",
    "\n",
    "train = pd.read_csv('../input/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../input/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "02fe9fb0f510b6c13dd34f30c16bb84314373b91"
   },
   "outputs": [],
   "source": [
    "train_text = train['question_text']\n",
    "test_text = test['question_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "# Create tfidf vectorizer\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True, # Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n",
    "    strip_accents='unicode', # normalization according unicodedata.normalize method\n",
    "    analyzer='char_wb', # Whether the feature should be made of word or character n-grams\n",
    "    # Only used if analyzer == 'word', but with 'char_wb' witout this token pattern works too long\n",
    "    # without 'char_wb' - too low F1 score, ~0.43\n",
    "    token_pattern=r'\\w{1,}', \n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 5), # ngram_range in [left,right]\n",
    "    max_features=50000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a0a4a140a049583ba6fbfef39504ec34798ba00e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting Vectorizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=1,\n",
       "        ngram_range=(2, 5), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nFitting Vectorizer')\n",
    "char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "2313c716706853a7e70205cd3e92e947eb108200",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming Text\n"
     ]
    }
   ],
   "source": [
    "print('\\nTransforming Text')\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "a6c619c1be86a7b42fadc94d638bf52255a132dd"
   },
   "outputs": [],
   "source": [
    "# Function to run cross validation with the capability of averaging results over multiple seeds\n",
    "def kfold_sklearn(train_df, test_df,y, log_regr, num_folds, seeds):\n",
    "    print(\"Starting Model. Train shape: {}\".format(train_df.shape))\n",
    "    oofs = np.zeros(train_df.shape[0])\n",
    "    subs = np.zeros(test_df.shape[0])\n",
    "    # Cross validation model\n",
    "    for seed in seeds:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        # Create arrays and dataframes to store results\n",
    "        oof_preds = np.zeros(train_df.shape[0])\n",
    "        sub_preds = np.zeros(test_df.shape[0])\n",
    "        \n",
    "        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y)):\n",
    "\n",
    "            train_x, train_y = train_df[train_idx], y[train_idx]\n",
    "            valid_x, valid_y = train_df[valid_idx], y[valid_idx]\n",
    "\n",
    "            log_regr.fit(train_x, train_y)\n",
    "\n",
    "            oof_preds[valid_idx] = log_regr.predict_proba(valid_x)[:,1]\n",
    "            sub_preds += log_regr.predict_proba(test_df)[:,1] / folds.n_splits\n",
    "\n",
    "            print('Fold %2d F1 Score : %.6f' % (n_fold + 1, f1_score(valid_y, (oof_preds[valid_idx]>0.27).astype(np.int))))\n",
    "            del train_x, train_y, valid_x, valid_y\n",
    "            gc.collect()\n",
    "\n",
    "        oofs += oof_preds/len(seeds)\n",
    "        subs += sub_preds/len(seeds)\n",
    "    \n",
    "    print('Full F1 score %.6f' % f1_score(y, (oofs>.27).astype(np.int)))\n",
    "    return oofs, subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "39bd997c971f248093a9743e79d102214d837581",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model. Train shape: (1306122, 50000)\n",
      "Fold  1 F1 Score : 0.631169\n",
      "Fold  2 F1 Score : 0.627479\n",
      "Fold  3 F1 Score : 0.636799\n",
      "Fold  4 F1 Score : 0.626820\n",
      "Fold  5 F1 Score : 0.629956\n",
      "Full F1 score 0.630454\n"
     ]
    }
   ],
   "source": [
    "log_regr = LogisticRegression(C=20, solver='sag') # solver - Algorithm to use in the optimization problem. ‘sag’ solver for multiclass problems\n",
    "kfold_sklearn(train_char_features, test_char_features, y, log_regr, 5, [42])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "169854a201e9d6418d5ef6d2d04fe1c1bd2c557b"
   },
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c4efd5b297da84072c408d711c2ddff9c4b6456f"
   },
   "source": [
    "### <a name=\"summary\">Summary</a>\n",
    "\n",
    "[Back to content list](#content_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36ffcf91fb3e327930abf6626b87cc6c5a62e485"
   },
   "source": [
    "## 1. Preprocessing of text data\n",
    "\n",
    "With help of some kernels I found out really nice information about data preprocessing for embeddings, which we can see in implementation.\n",
    "\n",
    "I preprocessed data for most popular 3 embeddings which I saw in examples just for convinience in the second step.\n",
    "\n",
    "## 2. Simple feed-forward neural network model\n",
    "\n",
    "I used example from the lesson for simple feed-forward nn model and decreased dimensions with tensor.view().\n",
    "\n",
    "I received low F1 score, ~0,11\n",
    "\n",
    "Why f1 score?\n",
    "\n",
    "I think that it is a good metric because it is the harmonic mean between precision(точність) and recall(повнота).\n",
    "\n",
    "Don't realize, why accuracy so low, I preprocessed df for embeddings specific embeddings, \n",
    "I tried to:\n",
    "- Create new layer 100, 10\n",
    "- Increase epochs count to 20\n",
    "\n",
    "It didn't help to increase F1 score and decrease the loss for NN model.\n",
    "\n",
    "## 3. TF-IDF vectorizer and Logistic regression\n",
    "\n",
    "For 3rd task I used tf-idf vectorizer + logistic regression. \n",
    "\n",
    "I have an experience with logistic regression and logistic regression works more clear for me than rnn.\n",
    "\n",
    "F1 score > 0.6, I think that's good result, because a lot of kernels with rnn or logistic regression have f1 score ~0.6\n",
    "\n",
    "But it can be better, we can try to:\n",
    "- Change TfIdf vectorizer settings, espessially try to use another token pattern, analyzer, etc. \n",
    "- Change C parameter\n",
    "- Change random state(seeds) parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
